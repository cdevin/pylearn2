from __future__ import division
import time
import numpy as np
from theano import function
import theano.tensor as T
from pylearn2.monitor import Monitor
from pylearn2.training_algorithms.training_algorithm import TrainingAlgorithm
import pylearn2.costs.cost
from pylearn2.utils import sharedX
from theano.printing import Print
from pylearn2.training_callbacks.training_callback import TrainingCallback
import warnings
from theano import config

class DropOut_SGD(TrainingAlgorithm):
    """
    Stochastic Gradient Descent

    WRITEME: what is a good reference to read about this algorithm?

    A TrainingAlgorithm that does gradient descent on minibatches.

    """

    def setup(self, model, dataset):
        self.model = model

        batch_size = self.batch_size
        if hasattr(model, "force_batch_size"):
            if model.force_batch_size > 0:
                if batch_size is not None:
                    if batch_size != model.force_batch_size:
                        if self.set_batch_size:
                            model.set_batch_size(batch_size)
                        else:
                            raise ValueError("batch_size argument to SGD conflicts with model's force_batch_size attribute")
                else:
                    self.batch_size = model.force_batch_size
        self.monitor = Monitor.get_monitor(model)
        # TODO: come up with some standard scheme for associating training runs
        # with monitors / pushing the monitor automatically, instead of just
        # enforcing that people have called push_monitor
        assert self.monitor.get_examples_seen() == 0
        # TODO: monitoring batch size ought to be configurable
        # separately from training batch size, e.g. if you would rather
        # monitor on one somewhat big batch but update on many small
        # batches.
        # IG adds note: yes, but the default should be for them to be
        # the same. Theano convolution has a hard-coded batch size, so
        # if you have a convolutional model you can't just go changing
        # the batch size everywhere, and the code should make it easy
        # to have a fixed batch size.
        if self.monitoring_dataset is not None:
            self.monitor.add_dataset(dataset=self.monitoring_dataset,
                                 mode='sequential',
                                 batch_size=self.batch_size,
                                 num_batches=self.monitoring_batches)
        self.monitor._sanity_check()




        X = model.get_input_space().make_theano_batch(name="%s[X]" % self.__class__.__name__)
        self.topo = not X.ndim == 2
        Y = T.matrix(name="%s[Y]" % self.__class__.__name__)

        try:
            iter(self.cost)
            iterable_cost = True
        except TypeError:
            iterable_cost = False
        if iterable_cost:
            cost_value = 0
            self.supervised = False
            for c in self.cost:
                if (isinstance(c, pylearn2.costs.cost.SupervisedCost)):
                    self.supervised = True
                    cost_value += c(model, X, Y)
                else:
                    cost_value += c(model, X)
            #cost_value = sum(c(model, X) for c in self.cost)
        else:
            if self.cost.supervised:
                self.supervised = True
                cost_value = self.cost(model, X, Y)
            else:
                self.supervised = False
                cost_value = self.cost(model, X)
        if cost_value.name is None:
            if self.supervised:
                cost_value.name = 'sgd_cost(' + X.name + ', ' + Y.name + ')'
            else:
                cost_value.name = 'sgd_cost(' + X.name + ')'

        # Set up monitor to model the objective value, learning rate,
        # momentum (if applicable), and extra channels defined by
        # the cost
        # TODO: also monitor things defined by the model
        learning_rate = self.learning_rate
        if self.monitoring_dataset is not None:
            if self.supervised:
                cost_channels = self.cost.get_monitoring_channels(model, X, Y)
                ipt = (X, Y)
            else:
                cost_channels = self.cost.get_monitoring_channels(model, X)
                ipt = X
            # These channel names must not vary, since callbacks that respond to the
            # values in the monitor use the name to find them
            self.monitor.add_channel(name='sgd_cost', ipt=ipt,
                    val=cost_value, dataset=self.monitoring_dataset)
            self.monitor.add_channel(name='learning_rate', ipt=ipt,
                    val=learning_rate, dataset=self.monitoring_dataset)
            for key in cost_channels:
                self.monitor.add_channel(name=key, ipt=ipt,
                        val=cost_channels[key], dataset=self.monitoring_dataset)
            if self.momentum:
                self.monitor.add_channel(name='momentum', ipt=ipt,
                        val=self.momentum, dataset=self.monitoring_dataset)

        params = list(model.get_params())
        assert len(params) > 0
        for i, param in enumerate(params):
            if param.name is None:
                param.name = 'sgd_params[%d]' % i
        grads = dict(zip(params, T.grad(cost_value, params, disconnected_inputs = 'warn')))
        for param in grads:
            if grads[param].name is None:
                grads[param].name = ('grad(%(costname)s, %(paramname)s)' %
                                     {'costname': cost_value.name,
                                      'paramname': param.name})

        lr_scalers = model.get_lr_scalers()

        for key in lr_scalers:
            if key not in params:
                raise ValueError("Tried to scale the learning rate on " +\
                        str(key)+" which is not an optimization parameter.")

        print 'Parameter and initial learning rate summary:'
        for param in params:
            param_name = param.name
            if param_name is None:
                param_name = 'anon_param'
            lr = learning_rate.get_value() * lr_scalers.get(param,1.)
            print '\t'+param_name+': '+str(lr)

        if self.momentum is None:
            updates = dict(zip(params, [param - learning_rate * \
                lr_scalers.get(param, 1.) * grads[param]
                                    for param in params]))
        else:
            updates = {}
            for param in params:
                inc = sharedX(param.get_value() * 0.)
                if param.name is not None:
                    inc.name = 'inc_'+param.name
                updated_inc = self.momentum * inc - learning_rate * grads[param]
                updates[inc] = updated_inc
                updates[param] = param + updated_inc


        for param in updates:
            if updates[param].name is None:
                updates[param].name = 'sgd_update(' + param.name + ')'
        model.censor_updates(updates)
        for param in updates:
            if updates[param].name is None:
                updates[param].name = 'censor(sgd_update(' + param.name + '))'

        if self.supervised:
            self.sgd_update = function([X, Y], updates=updates,
                                   name='sgd_update')
        else:
            self.sgd_update = function([X], updates=updates,
                                   name='sgd_update')
        self.params = params



