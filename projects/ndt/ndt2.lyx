#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
Suppose at each node of the tree we minimize the expected entropy of classes
 post-split:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\mathbb{E}_{s\sim p}[\mathcal{H}(c\mid s)]
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
=\sum_{s}(\frac{1}{m}\sum_{x}p(s\mid x))\mathcal{H}(c\mid s)
\]

\end_inset


\end_layout

\begin_layout Standard
Here the 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\mathcal{H}(c\mid s)$
\end_inset

is not independent of 
\begin_inset Formula $p(s\mid x)$
\end_inset

, for each combinatorial number of splits we will have a separate entropy.
 So the correct information gain would be:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
IG=\sum_{s_{1}..s_{n}}\prod_{i=1}^{n}p(s_{i}\mid x_{i})\sum_{k=0}^{1}\sum_{c}p(c\mid k,s)logp(c\mid k,s)
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
p(c\mid k,s)=\frac{\sum_{i}^{n}\mathbf{1}_{s_{i}=k}\mathbf{1}_{y_{i}=c}}{\sum_{i}^{n}\mathbf{1}_{s_{i}=k}}
\]

\end_inset


\end_layout

\begin_layout Standard
K is the branch number.
 And the sum over 
\begin_inset Formula $s_{i}$
\end_inset

would be intractable .
\end_layout

\begin_layout Standard
So instead let's have another neural net with MLL objective function like:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\sum_{n}\sum_{x,y}p(n\mid x)log\sum_{s}P(c=y\mid s)p_{\theta}(s\mid x)
\]

\end_inset


\end_layout

\begin_layout Standard
In which:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
P(c=i\mid s)=\frac{\frac{1}{n}\sum_{x,y}\mathbf{1}_{y=i}P(s\mid x)}{\sum_{i}i}
\]

\end_inset


\end_layout

\end_body
\end_document
