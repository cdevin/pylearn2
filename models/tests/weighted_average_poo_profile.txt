(3, 30, 30, 10)

ProfileMode.print_summary()
---------------------------

Time since import 4.564s
Theano compile time: 0.000s (0.0% since import)
    Optimization time: 0.000s
    Linker time: 0.000s
Theano fct call 0.004s (0.1% since import)
   Theano Op time 0.004s 0.1%(since import) 94.6%(of fct call)
   Theano function overhead in ProfileMode 0.000s 0.0%(since import) 5.4%(of fct call)
1 Theano fct call, 0.004s per call
Rest of the time since import 4.560s 99.9%

Theano fct summary:
<% total fct time> <total time> <time per call> <nb call> <fct name>
   100.0% 0.004s 4.01e-03s 1 None

Single Op-wise summary:
<% of local_time spent on this kind of Op> <cumulative %> <self seconds> <cumulative seconds> <time per call> [*] <nb_call> <nb_op> <nb_apply> <Op name>
   64.0%   64.0%  0.002s  0.002s  2.43e-03s *     1  1  1 <class 'theano.sandbox.cuda.basic_ops.GpuFromHost'>
   12.5%   76.5%  0.000s  0.003s  2.38e-04s *     2  1  2 <class 'theano.sandbox.cuda.basic_ops.GpuAlloc'>
   12.3%   88.8%  0.000s  0.003s  4.66e-04s *     1  1  1 <class 'theano.sandbox.cuda.basic_ops.HostFromGpu'>
    4.0%   92.8%  0.000s  0.004s  1.51e-05s *    10 10 10 <class 'theano.sandbox.cuda.basic_ops.GpuIncSubtensor'>
    2.3%   95.1%  0.000s  0.004s  4.30e-05s       2  1  2 <class 'theano.tensor.opt.MakeVector'>
    1.4%   96.5%  0.000s  0.004s  1.36e-05s       4  2  4 <class 'theano.sandbox.cuda.basic_ops.GpuReshape'>
    1.2%   97.6%  0.000s  0.004s  2.19e-05s *     2  2  2 <class 'theano.sandbox.cuda.basic_ops.GpuElemwise'>
    1.0%   98.7%  0.000s  0.004s  1.99e-05s *     2  1  2 <class 'theano.sandbox.cuda.basic_ops.GpuCAReduce'>
    0.9%   99.6%  0.000s  0.004s  3.84e-06s *     9  9  9 <class 'theano.sandbox.cuda.basic_ops.GpuSubtensor'>
    0.2%   99.8%  0.000s  0.004s  4.05e-06s *     2  2  2 <class 'theano.tensor.opt.Shape_i'>
    0.1%   99.9%  0.000s  0.004s  4.05e-06s *     1  1  1 <class 'theano.sandbox.cuda.basic_ops.GpuDimShuffle'>
    0.1%  100.0%  0.000s  0.004s  2.86e-06s *     1  1  1 <class 'theano.tensor.elemwise.Elemwise'>
   ... (remaining 0 single Op account for 0.00%(0.00s) of the runtime)
(*) Op is running a c implementation

Op-wise summary:
<% of local_time spent on this kind of Op> <cumulative %> <self seconds> <cumulative seconds> <time per call> [*]  <nb_call> <nb apply> <Op name>
   64.0%   64.0%  0.002s  0.002s  2.43e-03s *     1  1 GpuFromHost
   12.5%   76.5%  0.000s  0.003s  2.38e-04s *     2  2 GpuAlloc{memset_0=True}
   12.3%   88.8%  0.000s  0.003s  4.66e-04s *     1  1 HostFromGpu
    2.3%   91.1%  0.000s  0.003s  4.30e-05s       2  2 MakeVector
    1.5%   92.5%  0.000s  0.004s  5.60e-05s *     1  1 GpuIncSubtensor{InplaceSet;::, 0:32:, 0:32:, ::}
    1.0%   93.6%  0.000s  0.004s  1.99e-05s *     2  2 GpuCAReduce{add}{0,1,0}
    0.8%   94.4%  0.000s  0.004s  1.54e-05s       2  2 GpuReshape{3}
    0.7%   95.1%  0.000s  0.004s  2.69e-05s *     1  1 GpuElemwise{Composite{[Cast{float32}(EQ(i0, i1))]},no_inplace}
    0.6%   95.7%  0.000s  0.004s  1.19e-05s       2  2 GpuReshape{4}
    0.4%   96.2%  0.000s  0.004s  1.69e-05s *     1  1 GpuElemwise{Composite{[Composite{[mul(i0, true_div(i0, i1))]}(i0, Switch(i1, i2, i3))]}}[(0, 0)]
    0.4%   96.6%  0.000s  0.004s  1.41e-05s *     1  1 GpuIncSubtensor{InplaceSet;::, ::, ::, 0, 0, ::}
    0.3%   96.8%  0.000s  0.004s  1.10e-05s *     1  1 GpuIncSubtensor{InplaceSet;::, ::, ::, 0, 1, ::}
    0.3%   97.1%  0.000s  0.004s  1.00e-05s *     1  1 GpuIncSubtensor{InplaceSet;::, ::, ::, 1, 2, ::}
    0.3%   97.4%  0.000s  0.004s  1.00e-05s *     1  1 GpuIncSubtensor{InplaceSet;::, ::, ::, 2, 0, ::}
    0.3%   97.6%  0.000s  0.004s  1.00e-05s *     1  1 GpuIncSubtensor{InplaceSet;::, ::, ::, 1, 1, ::}
    0.3%   97.9%  0.000s  0.004s  1.00e-05s *     1  1 GpuIncSubtensor{InplaceSet;::, ::, ::, 2, 2, ::}
    0.3%   98.2%  0.000s  0.004s  1.00e-05s *     1  1 GpuIncSubtensor{InplaceSet;::, ::, ::, 2, 1, ::}
    0.3%   98.4%  0.000s  0.004s  1.00e-05s *     1  1 GpuIncSubtensor{InplaceSet;::, ::, ::, 1, 0, ::}
    0.3%   98.7%  0.000s  0.004s  1.00e-05s *     1  1 GpuIncSubtensor{InplaceSet;::, ::, ::, 0, 2, ::}
    0.2%   98.9%  0.000s  0.004s  9.06e-06s *     1  1 GpuSubtensor{::, 2:32:1, 1:31:1, ::}
   ... (remaining 12 Op account for   1.07%(0.00s) of the runtime)
(*) Op is running a c implementation

Apply-wise summary:
<% of local_time spent at this position> <cumulative %%> <apply time> <cumulative seconds> <time per call> [*] <nb_call> <Apply position> <Apply Op name>
   64.0%   64.0%  0.002s  0.002s 2.43e-03s  * 1   2 GpuFromHost(<TensorType(float32, 4D)>)
   12.3%   76.3%  0.000s  0.003s 4.66e-04s  * 1  36 HostFromGpu(GpuReshape{4}.0)
   11.6%   87.9%  0.000s  0.003s 4.39e-04s  * 1   5 GpuAlloc{memset_0=True}(CudaNdarrayConstant{[[[[[[ 0.]]]]]]}, Shape_i{0}.0, TensorConstant{30}, TensorConstant{30}, TensorConstant{3}, TensorConstant{3}, Shape_i{3}.0)
    1.5%   89.4%  0.000s  0.003s 5.70e-05s    1   3 MakeVector(Shape_i{0}.0, TensorConstant{30}, TensorConstant{30}, Shape_i{3}.0)
    1.5%   90.8%  0.000s  0.003s 5.60e-05s  * 1   7 GpuIncSubtensor{InplaceSet;::, 0:32:, 0:32:, ::}(GpuAlloc{memset_0=True}.0, GpuFromHost.0)
    0.9%   91.8%  0.000s  0.003s 3.60e-05s  * 1   4 GpuAlloc{memset_0=True}(CudaNdarrayConstant{[[[[ 0.]]]]}, Shape_i{0}.0, TensorConstant{32}, TensorConstant{32}, Shape_i{3}.0)
    0.8%   92.5%  0.000s  0.004s 2.91e-05s    1   8 MakeVector(Elemwise{Composite{[mul(mul(i0, i1), i1)]}}[(0, 0)].0, TensorConstant{9}, Shape_i{3}.0)
    0.7%   93.3%  0.000s  0.004s 2.69e-05s  * 1  31 GpuElemwise{Composite{[Cast{float32}(EQ(i0, i1))]},no_inplace}(GpuDimShuffle{0,1,2,x,x,3}.0, CudaNdarrayConstant{[[[[[[ 0.]]]]]]})
    0.6%   93.8%  0.000s  0.004s 2.19e-05s    1  27 GpuReshape{3}(GpuIncSubtensor{InplaceSet;::, ::, ::, 2, 2, ::}.0, MakeVector.0)
    0.5%   94.4%  0.000s  0.004s 2.00e-05s  * 1  34 GpuCAReduce{add}{0,1,0}(GpuReshape{3}.0)
    0.5%   94.9%  0.000s  0.004s 1.98e-05s  * 1  28 GpuCAReduce{add}{0,1,0}(GpuReshape{3}.0)
    0.4%   95.3%  0.000s  0.004s 1.69e-05s  * 1  32 GpuElemwise{Composite{[Composite{[mul(i0, true_div(i0, i1))]}(i0, Switch(i1, i2, i3))]}}[(0, 0)](GpuIncSubtensor{InplaceSet;::, ::, ::, 2, 2, ::}.0, GpuElemwise{Composite{[Cast{float32}(EQ(i0, i1))]},no_inplace}.0, CudaNdarrayConstant{[[[[[[ 1.]]]]]]}, GpuDimShuffle{0,1,2,x,x,3}.0)
    0.4%   95.7%  0.000s  0.004s 1.41e-05s  * 1  18 GpuIncSubtensor{InplaceSet;::, ::, ::, 0, 0, ::}(GpuAlloc{memset_0=True}.0, GpuSubtensor{::, 0:30:1, 0:30:1, ::}.0)
    0.4%   96.1%  0.000s  0.004s 1.38e-05s    1  35 GpuReshape{4}(GpuCAReduce{add}{0,1,0}.0, MakeVector.0)
    0.3%   96.4%  0.000s  0.004s 1.10e-05s  * 1  19 GpuIncSubtensor{InplaceSet;::, ::, ::, 0, 1, ::}(GpuIncSubtensor{InplaceSet;::, ::, ::, 0, 0, ::}.0, GpuSubtensor{::, 0:30:1, 1:31:1, ::}.0)
   ... (remaining 22 Apply instances account for 3.65%(0.00s) of the runtime)
(*) Op is running a c implementation

Some info useful for gpu:

    Spent 0.001s(14.829%) in cpu Op, 0.003s(85.171%) in gpu Op and 0.000s(0.000%) transfert Op

    Theano function input that are float64
    <fct name> <input name> <input type> <str input>

    List of apply that don't have float64 as input but have float64 in outputs
    (Useful to know if we forgot some cast when using floatX=float32 or gpu code)
    <Apply> <Apply position> <fct name> <inputs type> <outputs type>

Profile of Theano functions memory:
(This check only the output of each apply node. It don't check the temporary memory used by the op in the apply node.)
   We skipped 1 theano function(s). Each of them used less then 1024B(theano flags ProfileMode.min_memory_size) of total intermediate memory size

Here are tips to potentially make your code run faster
(if you think of new ones, suggest them on the mailing list).
Test them first, as they are not guaranteed to always provide a speedup.
  Sorry, no tip for today.
